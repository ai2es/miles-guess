{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5573a4bc-5463-40d8-a75a-6aaef8fcd06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/schreck/miniconda3/envs/evidential/lib/python3.8/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "from mlguess.torch.class_losses import relu_evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516cbd75-5da1-435b-b995-320004fa63ca",
   "metadata": {},
   "source": [
    "### Example usage for K-class problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c4e448-b5ec-4355-bf8f-5cc300cc8bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNABert(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(DNABert, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # note how we only take one hidden state from the sequeunce, which corresponds with the CLS token\n",
    "        cls_hidden_state = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        out = self.fc(cls_hidden_state)\n",
    "        return out\n",
    "    \n",
    "    def predict_uncertainty(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        y_pred = self(input_ids, attention_mask, token_type_ids)\n",
    "        \n",
    "        # dempster-shafer theory\n",
    "        evidence = relu_evidence(y_pred) # can also try softplus and exp evidence schemes\n",
    "        alpha = evidence + 1\n",
    "        S = torch.sum(alpha, dim=1, keepdim=True)\n",
    "        u = self.n_classes / S\n",
    "        prob = alpha / S\n",
    "        \n",
    "        # law of total uncertainty \n",
    "        epistemic = prob * (1 - prob) / (S + 1)\n",
    "        aleatoric = prob - prob**2 - epistemic\n",
    "        return prob, u, aleatoric, epistemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f642e42e-3083-454b-9720-71a78eb04061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "num_classes = 10\n",
    "\n",
    "model = DNABert(n_classes=num_classes)\n",
    "\n",
    "dna_sequence = \"AGCTAGCTAGCT\"\n",
    "\n",
    "# We need to convert the DNA sequence to the format expected by BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "inputs = tokenizer(dna_sequence, return_tensors='pt')\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc73cdee-d1f0-409d-bd01-57bfdc80cf46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 12943, 25572, 18195, 15900,  6593,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a6e6872-28a6-4ac3-b9d9-504792251f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5873,  0.1587,  0.2231, -0.0498,  0.5132, -0.6980, -0.0172, -0.1390,\n",
       "          0.7385,  0.0981]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "439bc25b-328f-4cd1-a5aa-bc5f9909da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob, u, aleatoric, epistemic = model.predict_uncertainty(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d8e5517-a7bb-4c9c-824b-a69e2ecdbca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1289, 0.0941, 0.0993, 0.0812, 0.1228, 0.0812, 0.0812, 0.0812, 0.1411,\n",
       "         0.0891]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a886205b-5f16-4255-a6d0-f947e9c17395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8118]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf79791-c511-4c6d-960c-b100794181f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1038, 0.0788, 0.0827, 0.0690, 0.0997, 0.0690, 0.0690, 0.0690, 0.1121,\n",
       "         0.0751]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aleatoric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27842698-60a9-419e-82d2-36b5ee341983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0084, 0.0064, 0.0067, 0.0056, 0.0081, 0.0056, 0.0056, 0.0056, 0.0091,\n",
       "         0.0061]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epistemic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ba8eec-26df-4c24-84a0-141d1caaa28b",
   "metadata": {},
   "source": [
    "### Evidential loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cffcfe27-be85-4948-8ba0-f10502e2f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlguess.torch.class_losses import edl_digamma_loss, edl_log_loss, edl_mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed550265-225f-4720-af09-1be5a86a8aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = \"digamma\"\n",
    "annealing_coefficient = 10.\n",
    "epoch = 0\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fee00e6d-be82-40d5-a555-ed2417bca50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if loss == \"digamma\":\n",
    "    criterion = edl_digamma_loss\n",
    "elif loss == \"log\":\n",
    "    criterion = edl_log_loss\n",
    "elif loss == \"mse\":\n",
    "    criterion = edl_mse_loss\n",
    "else:\n",
    "    logging.error(\"--uncertainty requires --mse, --log or --digamma.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69ccc5a0-a560-40d5-b9c2-6c303c8ba238",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_hot = torch.tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "loss = criterion(\n",
    "    outputs,\n",
    "    y_true_hot.float(), \n",
    "    epoch, \n",
    "    num_classes, \n",
    "    annealing_coefficient, \n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b902b09-1047-45d7-a233-2b48ed026481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3549, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bb75bb5-a005-4c44-8c4e-65026a13cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss.backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986ac65-8af1-447a-bae1-1528dbc6087b",
   "metadata": {},
   "source": [
    "### Regression example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46b170be-c45d-4ccb-af46-0916ddcbff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9307ad7c-8530-4ae9-8e7a-0e1c0e389f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNormalGamma(nn.Module):\n",
    "    def __init__(self, in_chanels, out_channels):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_chanels, out_channels*4)\n",
    "\n",
    "    def evidence(self, x):\n",
    "        return  torch.log(torch.exp(x) + 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred = self.linear(x).view(x.shape[0], -1, 4)\n",
    "        mu, logv, logalpha, logbeta = [w.squeeze(-1) for w in torch.split(pred, 1, dim=-1)]\n",
    "        return mu, self.evidence(logv), self.evidence(logalpha) + 1, self.evidence(logbeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d35247de-2f5c-47c8-9219-ef7de17fd1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNABertRegressor(nn.Module):\n",
    "    def __init__(self, n_tasks, training_var = [1.0]):\n",
    "        super(DNABertRegressor, self).__init__()\n",
    "        self.output_size = n_tasks\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc = LinearNormalGamma(self.bert.config.hidden_size, self.output_size)\n",
    "        self.training_var = training_var\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # note how we only take one hidden state from the sequeunce, which corresponds with the CLS token\n",
    "        cls_hidden_state = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        out = self.fc(cls_hidden_state)\n",
    "        return out\n",
    "\n",
    "    def predict_uncertainty(self, input_ids, attention_mask, token_type_ids=None, y_scaler=None):\n",
    "        mu, v, alpha, beta = self(input_ids, attention_mask, token_type_ids)\n",
    "        aleatoric = beta / (alpha - 1)\n",
    "        epistemic = beta / (v * (alpha - 1))\n",
    "\n",
    "        if len(mu.shape) == 1:\n",
    "            mu = np.expand_dims(mu, 1)\n",
    "            aleatoric = np.expand_dims(aleatoric, 1)\n",
    "            epistemic = np.expand_dims(epistemic, 1)\n",
    "\n",
    "        if y_scaler:\n",
    "            mu = y_scaler.inverse_transform(mu)\n",
    "\n",
    "        for i in range(mu.shape[-1]):\n",
    "            aleatoric[:, i] *= self.training_var[i]\n",
    "            epistemic[:, i] *= self.training_var[i]\n",
    "\n",
    "        return mu, aleatoric, epistemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "18bd5445-89c6-4577-ac9e-7f52a286eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma, nu, alpha, beta = pred\n",
    "# loss = nll_loss(gamma, nu, alpha, beta, labels)\n",
    "# loss += reg(gamma, nu, alpha, beta, labels)\n",
    "# loss += mmse_loss(gamma, nu, alpha, beta, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0832e62-d09a-427c-a67a-e29e1c70c290",
   "metadata": {},
   "source": [
    "##### Compute binder score training variance: np.var(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0664a85-1fc6-4801-bf19-0608400150ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNABertRegressor(n_tasks=1, training_var = [0.0033433])\n",
    "\n",
    "dna_sequence = \"AGCTAGCTAGCT\"\n",
    "\n",
    "# We need to convert the DNA sequence to the format expected by BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "inputs = tokenizer(dna_sequence, return_tensors='pt')\n",
    "\n",
    "# Forward pass through the model\n",
    "gamma, nu, alpha, beta = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95df513b-b9b4-44a1-bedb-4cb83ff5f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, a, e = model.predict_uncertainty(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "470995a8-69c5-4ab4-b777-45ba1e84b164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2118]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de154324-e57b-4842-87f9-81c890088d6d",
   "metadata": {},
   "source": [
    "### Regression losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "431438fc-a445-4021-8287-45e3f9383393",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-8\n",
    "\n",
    "def modified_mse(gamma, nu, alpha, beta, target, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Lipschitz MSE loss of the \"Improving evidential deep learning via multitask learning.\"\n",
    "\n",
    "    Reference: https://www.mit.edu/~amini/pubs/pdf/deep-evidential-regression.pdf\n",
    "    Source: https://github.com/deargen/MT-ENet/tree/468822188f52e517b1ee8e386eea607b2b7d8829\n",
    "\n",
    "    Args:\n",
    "        gamma ([FloatTensor]): the output of the ENet.\n",
    "        nu ([FloatTensor]): the output of the ENet.\n",
    "        alpha ([FloatTensor]): the output of the ENet.\n",
    "        beta ([FloatTensor]): the output of the ENet.\n",
    "        target ([FloatTensor]): true labels.\n",
    "        reduction (str, optional): . Defaults to 'mean'.\n",
    "    Returns:\n",
    "        [FloatTensor]: The loss value. \n",
    "    \"\"\"\n",
    "    mse = (gamma-target)**2\n",
    "    c = get_mse_coef(gamma, nu, alpha, beta, target).detach()\n",
    "    mod_mse = mse*c\n",
    "    \n",
    "    if reduction == 'mean': \n",
    "        return mod_mse.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return mod_mse.sum()\n",
    "    else:\n",
    "        return mod_mse\n",
    "\n",
    "def get_mse_coef(gamma, nu, alpha, beta, y):\n",
    "    \"\"\"\n",
    "    Return the coefficient of the MSE loss for each prediction.\n",
    "    By assigning the coefficient to each MSE value, it clips the gradient of the MSE\n",
    "    based on the threshold values U_nu, U_alpha, which are calculated by check_mse_efficiency_* functions.\n",
    "\n",
    "    Reference: https://www.mit.edu/~amini/pubs/pdf/deep-evidential-regression.pdf\n",
    "    Source: https://github.com/deargen/MT-ENet/tree/468822188f52e517b1ee8e386eea607b2b7d8829\n",
    "\n",
    "    Args:\n",
    "        gamma ([FloatTensor]): the output of the ENet.\n",
    "        nu ([FloatTensor]): the output of the ENet.\n",
    "        alpha ([FloatTensor]): the output of the ENet.\n",
    "        beta ([FloatTensor]): the output of the ENet.\n",
    "        y ([FloatTensor]): true labels.\n",
    "    Returns:\n",
    "        [FloatTensor]: [0.0-1.0], the coefficient of the MSE for each prediction.\n",
    "    \"\"\"\n",
    "    alpha_eff = check_mse_efficiency_alpha(nu, alpha, beta)\n",
    "    nu_eff = check_mse_efficiency_nu(gamma, nu, alpha, beta)\n",
    "    delta = (gamma - y).abs()\n",
    "    min_bound = torch.min(nu_eff, alpha_eff).min()\n",
    "    c = (min_bound.sqrt()/(delta + tol)).detach()\n",
    "    return torch.clip(c, min=False, max=1.)\n",
    "\n",
    "\n",
    "def check_mse_efficiency_alpha(nu, alpha, beta):\n",
    "    \"\"\"\n",
    "    Check the MSE loss (gamma - y)^2 can make negative gradients for alpha, which is\n",
    "    a pseudo observation of the normal-inverse-gamma. We can use this to check the MSE\n",
    "    loss can success(increase the pseudo observation, alpha).\n",
    "\n",
    "    Reference: https://www.mit.edu/~amini/pubs/pdf/deep-evidential-regression.pdf\n",
    "    Source: https://github.com/deargen/MT-ENet/tree/468822188f52e517b1ee8e386eea607b2b7d8829\n",
    "\n",
    "    Args:\n",
    "        nu (torch.Tensor): nu output value of the evidential network\n",
    "        alpha (torch.Tensor): alpha output value of the evidential network\n",
    "        beta (torch.Tensor): beta output value of the evidential network\n",
    "\n",
    "    Return:\n",
    "        partial f / partial alpha(numpy.array) \n",
    "        where f => the NLL loss (BayesianDTI.loss.MarginalLikelihood)\n",
    "    \n",
    "    \"\"\"\n",
    "    right = (torch.exp((torch.digamma(alpha+0.5)-torch.digamma(alpha))) - 1)*2*beta*(1+nu) / (nu + 1e-8)\n",
    "    return right.detach()\n",
    "\n",
    "\n",
    "def check_mse_efficiency_nu(gamma, nu, alpha, beta):\n",
    "    \"\"\"\n",
    "    Check the MSE loss (gamma - y)^2 can make negative gradients for nu, which is\n",
    "    a pseudo observation of the normal-inverse-gamma. We can use this to check the MSE\n",
    "    loss can success(increase the pseudo observation, nu).\n",
    "\n",
    "    Reference: https://www.mit.edu/~amini/pubs/pdf/deep-evidential-regression.pdf\n",
    "    Source: https://github.com/deargen/MT-ENet/tree/468822188f52e517b1ee8e386eea607b2b7d8829\n",
    "\n",
    "    Args:\n",
    "        gamma (torch.Tensor): gamma output value of the evidential network\n",
    "        nu (torch.Tensor): nu output value of the evidential network\n",
    "        alpha (torch.Tensor): alpha output value of the evidential network\n",
    "        beta (torch.Tensor): beta output value of the evidential network\n",
    "    \n",
    "    Return:\n",
    "        partial f / partial nu(torch.Tensor) \n",
    "        where f => the NLL loss (BayesianDTI.loss.MarginalLikelihood)\n",
    "    \"\"\"\n",
    "    gamma, nu, alpha, beta = gamma.detach(), nu.detach(), alpha.detach(), beta.detach()\n",
    "    nu_1 = (nu + 1) / (nu + tol)\n",
    "    return beta * nu_1 / (alpha + tol)\n",
    "\n",
    "        \n",
    "class EvidentialMarginalLikelihood(torch.nn.modules.loss._Loss):\n",
    "    \"\"\"\n",
    "    Marginal likelihood error of prior network.\n",
    "    The target value is not a distribution (mu, std), but a just value.\n",
    "    \n",
    "    This is a negative log marginal likelihood, with integral mu and sigma.\n",
    "\n",
    "    Reference: https://www.mit.edu/~amini/pubs/pdf/deep-evidential-regression.pdf\n",
    "    Source: https://github.com/deargen/MT-ENet/tree/468822188f52e517b1ee8e386eea607b2b7d8829\n",
    "    \"\"\"\n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean'):\n",
    "        super(EvidentialMarginalLikelihood, self).__init__(size_average, reduce, reduction)\n",
    "    \n",
    "    def forward(self, gamma: torch.Tensor, nu: torch.Tensor, alpha: torch.Tensor, beta: torch.Tensor,\n",
    "                target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gamma (torch.Tensor): gamma output value of the evidential network\n",
    "            nu (torch.Tensor): nu output value of the evidential network\n",
    "            alpha (torch.Tensor): alpha output value of the evidential network\n",
    "            beta (torch.Tensor): beta output value of the evidential network\n",
    "            target (torch.Tensor): target value\n",
    "            \n",
    "        Return:\n",
    "            (Tensor) Negative log marginal likelihood of EvidentialNet\n",
    "                p(y|m) = Student-t(y; gamma, (beta(1+nu))/(nu*alpha) , 2*alpha)\n",
    "                then, the negative log likelihood is (CAUTION QUITE COMPLEX!)\n",
    "                NLL = -log(p(y|m)) =\n",
    "                    log(3.14/nu)*0.5 - alpha*log(2*beta*(1 + nu)) + (alpha + 0.5)*log( nu(target - gamma)^2 + 2*beta(1 + nu) )\n",
    "                    + log(GammaFunc(alpha)/GammaFunc(alpha + 0.5))\n",
    "        \"\"\"\n",
    "        pi = torch.tensor(np.pi)\n",
    "        x1 = torch.log(pi/(nu + tol))*0.5\n",
    "        x2 = -alpha*torch.log(2.*beta*(1.+ nu) + tol)\n",
    "        x3 = (alpha + 0.5)*torch.log( nu*(target - gamma)**2 + 2.*beta*(1. + nu) + tol)\n",
    "        x4 = torch.lgamma(alpha + tol) - torch.lgamma(alpha + 0.5 + tol)\n",
    "        if self.reduction == 'mean': \n",
    "            return (x1 + x2 + x3 + x4).mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return (x1 + x2 + x3 + x4).sum()\n",
    "        else:\n",
    "            return x1 + x2 + x3 + x4\n",
    "\n",
    "class EvidenceRegularizer(torch.nn.modules.loss._Loss):\n",
    "    \"\"\"\n",
    "    Regularization for the regression prior network.\n",
    "    If self.factor increases, the model output the wider(high confidence interval) predictions.\n",
    "\n",
    "    Reference: https://www.mit.edu/~amini/pubs/pdf/deep-evidential-regression.pdf\n",
    "    Source: https://github.com/deargen/MT-ENet/tree/468822188f52e517b1ee8e386eea607b2b7d8829\n",
    "    \"\"\"\n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean', factor=0.1):\n",
    "        super(EvidenceRegularizer, self).__init__(size_average, reduce, reduction)\n",
    "        self.factor = factor\n",
    "    \n",
    "    def forward(self, gamma: torch.Tensor, nu: torch.Tensor, alpha: torch.Tensor,\n",
    "                target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gamma (torch.Tensor): gamma output value of the evidential network\n",
    "            nu (torch.Tensor): nu output value of the evidential network\n",
    "            alpha (torch.Tensor): alpha output value of the evidential network\n",
    "            target (torch.Tensor): target value\n",
    "\n",
    "        Return:\n",
    "            (Tensor) prior network regularization\n",
    "            Loss = |y - gamma|*(2*nu + alpha) * factor\n",
    "            \n",
    "        \"\"\"\n",
    "        loss_value =  torch.abs(target - gamma)*(2*nu + alpha) * self.factor\n",
    "        if self.reduction == 'mean': \n",
    "            return loss_value.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss_value.sum()\n",
    "        else:\n",
    "            return loss_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c955e253-b38d-49e1-8a9a-458d157f4dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.Tensor([1.45456457])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "64ed98f5-5e04-47e3-99dc-290663ad9c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = EvidentialMarginalLikelihood()(gamma, nu, alpha, beta, y_pred)\n",
    "loss += EvidenceRegularizer()(gamma, nu, alpha, y_pred)\n",
    "loss += modified_mse(gamma, nu, alpha, beta, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "054c0722-703d-4726-ae69-5ea9b0399189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.5012, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fdc5a7ca-7747-4c95-85cf-f4378a05c2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/schreck/miniconda3/envs/evidential/lib/python3.8/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "mae = torch.nn.L1Loss()(y_pred, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f2b8227b-181a-4558-910f-7dd670aa9d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6892, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebcdcfd-f06c-4928-abdb-62a16a9449f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-evidential]",
   "language": "python",
   "name": "conda-env-miniconda3-evidential-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
