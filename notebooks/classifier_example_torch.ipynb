{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c7e9fb8-ef7f-4f32-bad4-ffeafbf90cd9",
   "metadata": {},
   "source": [
    "# MILES-GUESS Classification Example Notebook (PyTorch)\n",
    "\n",
    "John Schreck, David John Gagne, Charlie Becker, Gabrielle Gantos, Dhamma Kimpara, Thomas Martin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f9b51-274a-4721-8d88-f71af5c37c1d",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "\n",
    "This notebook is meant to demonstrate a functional example of how to use the miles-guess repository for training an evidential classification model.\n",
    "\n",
    "Steps to get this notebook running:\n",
    "1) Follow package installation steps in the miles-guess [ReadMe](https://github.com/ai2es/miles-guess/tree/main).\n",
    "2) Run the cells in this notebook in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08d23f15-63a1-484d-b9be-e46b6995bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from mlguess.keras.data import load_ptype_uq, preprocess_data\n",
    "from mlguess.torch.models import CategoricalDNN\n",
    "from mlguess.torch.metrics import MetricsCalculator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from mlguess.torch.class_losses import edl_mse_loss, edl_digamma_loss, edl_log_loss, relu_evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b25e71d-5d5c-4491-9b9b-0644650ae6e7",
   "metadata": {},
   "source": [
    "### Load a config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2c8647-fea7-4f9c-8c8e-2a83fcfab9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/evidential_classifier_torch.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "499510c4-f4ed-4d44-a096-6290571c1639",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b9c45c-2e67-46ab-a7d1-4a94a0109005",
   "metadata": {},
   "source": [
    "### Load the training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc84304-c38c-4f0a-ab1b-e5ecac067a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = []\n",
    "for features in [\"TEMP_C\", \"T_DEWPOINT_C\", \"UGRD_m/s\", \"VGRD_m/s\"]:\n",
    "    input_features += conf[\"data\"][features]\n",
    "output_features = conf[\"data\"][\"ptypes\"]\n",
    "\n",
    "# Load data\n",
    "_conf = copy.deepcopy(conf)\n",
    "_conf.update(conf[\"data\"])\n",
    "data = load_ptype_uq(_conf, data_split=0, verbose=1, drop_mixed=False)\n",
    "# check if we should scale the input data by groups\n",
    "scale_groups = [] if \"scale_groups\" not in conf[\"data\"] else conf[\"data\"][\"scale_groups\"]\n",
    "groups = [list(conf[\"data\"][g]) for g in scale_groups]\n",
    "leftovers = list(\n",
    "    set(input_features)\n",
    "    - set([row for group in scale_groups for row in conf[\"data\"][group]])\n",
    ")\n",
    "if len(leftovers):\n",
    "    groups.append(leftovers)\n",
    "# scale the data\n",
    "scaled_data, scalers = preprocess_data(\n",
    "    data,\n",
    "    input_features,\n",
    "    output_features,\n",
    "    scaler_type=conf[\"data\"][\"scaler_type\"],\n",
    "    encoder_type=\"onehot\",\n",
    "    groups=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0cd02c7-3f02-4bf7-82b6-c783845bb4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_embedding(labels, num_classes=10):\n",
    "    # Convert to One Hot Encoding\n",
    "    y = torch.eye(num_classes)\n",
    "    return y[labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58870116-bf9d-4a0f-bc1b-255da8edae01",
   "metadata": {},
   "source": [
    "### Convert the pandas dataframe into torch tensors, wrap in Dataset then Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20843b47-26f0-4e0b-8cc1-72a11bdf50f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(scaled_data[\"train_x\"].values)\n",
    "y_train = torch.LongTensor(np.argmax(scaled_data[\"train_y\"], axis=1))\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0cf1ed-2254-4c57-9541-97cc6ae0e678",
   "metadata": {},
   "source": [
    "### First lets train a standard (non-evidential) classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7132121f-de6e-4091-a36c-bab56f34e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf[\"model\"][\"softmax\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3beb428-a58d-4e98-a89f-d80e1f0ef3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = CategoricalDNN(**conf[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "013c0efd-8fd3-4b8f-9284-15ebb22ef8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalDNN(\n",
       "  (fcn): Sequential(\n",
       "    (0): Linear(in_features=84, out_features=212, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.1167, inplace=False)\n",
       "    (3): Linear(in_features=212, out_features=212, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): Dropout(p=0.1167, inplace=False)\n",
       "    (6): Linear(in_features=212, out_features=212, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): Dropout(p=0.1167, inplace=False)\n",
       "    (9): Linear(in_features=212, out_features=212, bias=True)\n",
       "    (10): LeakyReLU(negative_slope=0.01)\n",
       "    (11): Dropout(p=0.1167, inplace=False)\n",
       "    (12): Linear(in_features=212, out_features=4, bias=True)\n",
       "    (13): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf7b16-cb9c-48da-b564-b6cf06a266f0",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dcf127c-9149-463a-bc36-1b6b7ae437e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.001)\n",
    "metrics = MetricsCalculator(use_uncertainty=False)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "batches_per_epoch = 10\n",
    "\n",
    "results_dict = defaultdict(list)\n",
    "for epoch in range(num_epochs):\n",
    "    mlp.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for k, (batch_X, batch_y) in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        outputs = mlp(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions += batch_y.size(0)\n",
    "        correct_predictions += (predicted == batch_y).float().mean().item()\n",
    "\n",
    "        metrics_dict = metrics(one_hot_embedding(batch_y, 4), outputs, split=\"train\")\n",
    "        for name, value in metrics_dict.items():\n",
    "            results_dict[name].append(value.item())\n",
    "    \n",
    "\n",
    "        if (k + 1) == batches_per_epoch:\n",
    "            break\n",
    "    \n",
    "    # Calculate epoch statistics\n",
    "    avg_loss = total_loss / batches_per_epoch\n",
    "    accuracy = correct_predictions / batches_per_epoch\n",
    "    \n",
    "    #print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b00298b1-83a4-4fa8-a98d-3c1850ea0fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csi 0.25876237462243046\n",
      "train_ave_acc 0.4687493490720337\n",
      "train_prec 0.42272548775507235\n",
      "train_recall 0.4687493490720337\n",
      "train_f1 0.4416411424933151\n",
      "train_auc 0.898654333499807\n"
     ]
    }
   ],
   "source": [
    "for key, val in results_dict.items():\n",
    "    print(key, np.mean(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8537a3c-ba74-47e5-a12b-49d62485c585",
   "metadata": {},
   "source": [
    "### Next lets train an evidential classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff24ebbf-b04a-41ea-91f0-51ecbcb5b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf[\"model\"][\"softmax\"] = False\n",
    "conf[\"model\"][\"lng\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "905f3c5f-045c-4441-ba59-db420a9b2264",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_mlp = CategoricalDNN(**conf[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7361134b-0860-4692-b57c-68d5f94b2d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalDNN(\n",
       "  (fcn): Sequential(\n",
       "    (0): Linear(in_features=84, out_features=212, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.1167, inplace=False)\n",
       "    (3): Linear(in_features=212, out_features=212, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): Dropout(p=0.1167, inplace=False)\n",
       "    (6): Linear(in_features=212, out_features=212, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): Dropout(p=0.1167, inplace=False)\n",
       "    (9): Linear(in_features=212, out_features=212, bias=True)\n",
       "    (10): LeakyReLU(negative_slope=0.01)\n",
       "    (11): Dropout(p=0.1167, inplace=False)\n",
       "    (12): Linear(in_features=212, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f06fec-b07e-4d08-b55a-12ead26c0cf7",
   "metadata": {},
   "source": [
    "### Note here there is no output activation\n",
    "### The other main difference is the choice of loss, seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35b4accb-706d-4bad-9f44-a502b3151b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_embedding(labels, num_classes=10):\n",
    "    # Convert to One Hot Encoding\n",
    "    y = torch.eye(num_classes)\n",
    "    return y[labels]\n",
    "\n",
    "criterion = edl_digamma_loss\n",
    "optimizer = optim.Adam(ev_mlp.parameters(), lr=0.001)\n",
    "metrics = MetricsCalculator(use_uncertainty=False)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "batches_per_epoch = 10\n",
    "num_classes = 4  # Assuming 4 classes based on your one_hot_embedding call\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ev_mlp.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_evidence = 0\n",
    "    total_evidence_succ = 0\n",
    "    total_evidence_fail = 0\n",
    "    total_uncertainty = 0\n",
    "    \n",
    "    for k, (batch_X, batch_y) in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        outputs = ev_mlp(batch_X)\n",
    "        batch_y_onehot = one_hot_embedding(batch_y, num_classes)\n",
    "        \n",
    "        loss = criterion(\n",
    "            outputs, batch_y_onehot.float(), epoch, num_classes, 10, batch_y.device\n",
    "        )\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        match = torch.eq(predicted, batch_y).float().view(-1, 1)\n",
    "        acc = torch.mean(match)\n",
    "        \n",
    "        evidence = relu_evidence(outputs)\n",
    "        alpha = evidence + 1\n",
    "        u = num_classes / torch.sum(alpha, dim=1, keepdim=True)\n",
    "        \n",
    "        total_evidence_batch = torch.sum(evidence, 1, keepdim=True)\n",
    "        mean_evidence = torch.mean(total_evidence_batch)\n",
    "        mean_evidence_succ = torch.sum(total_evidence_batch * match) / (torch.sum(match) + 1e-20)\n",
    "        mean_evidence_fail = torch.sum(total_evidence_batch * (1 - match)) / (torch.sum(1 - match) + 1e-20)\n",
    "        \n",
    "        total_acc += acc.item()\n",
    "        total_evidence += mean_evidence.item()\n",
    "        total_evidence_succ += mean_evidence_succ.item()\n",
    "        total_evidence_fail += mean_evidence_fail.item()\n",
    "        total_uncertainty += torch.mean(u).item()\n",
    "\n",
    "        metrics_dict = metrics(one_hot_embedding(batch_y, 4), outputs, split=\"train\")\n",
    "        for name, value in metrics_dict.items():\n",
    "            results_dict[name].append(value.item())\n",
    "        \n",
    "        if (k + 1) == batches_per_epoch:\n",
    "            break\n",
    "    \n",
    "    # Calculate epoch statistics\n",
    "    avg_loss = total_loss / batches_per_epoch\n",
    "    avg_acc = total_acc / batches_per_epoch\n",
    "    avg_evidence = total_evidence / batches_per_epoch\n",
    "    avg_evidence_succ = total_evidence_succ / batches_per_epoch\n",
    "    avg_evidence_fail = total_evidence_fail / batches_per_epoch\n",
    "    avg_uncertainty = total_uncertainty / batches_per_epoch\n",
    "    \n",
    "    # print(f'Epoch [{epoch+1}/{num_epochs}]:')\n",
    "    # print(f'  Loss: {avg_loss:.4f}')\n",
    "    # print(f'  Accuracy: {avg_acc:.4f}')\n",
    "    # print(f'  Mean Evidence: {avg_evidence:.4f}')\n",
    "    # print(f'  Mean Evidence (Correct): {avg_evidence_succ:.4f}')\n",
    "    # print(f'  Mean Evidence (Incorrect): {avg_evidence_fail:.4f}')\n",
    "    # print(f'  Mean Uncertainty: {avg_uncertainty:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bf9b627-4991-4187-8a10-1e0198951f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csi 0.25621067497624167\n",
      "train_ave_acc 0.467960801452284\n",
      "train_prec 0.4292476873831698\n",
      "train_recall 0.467960801452284\n",
      "train_f1 0.4421940509144526\n",
      "train_auc 0.8909479463346074\n"
     ]
    }
   ],
   "source": [
    "for key, val in results_dict.items():\n",
    "    print(key, np.mean(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3615ee-7c0a-4b58-94a4-0d7616475be7",
   "metadata": {},
   "source": [
    "### Thats it! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d14312-6a99-4940-87a8-41eb60c0bdfc",
   "metadata": {},
   "source": [
    "### Questions? Email John Schreck (schreck@ucar.edu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit",
   "language": "python",
   "name": "credit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
