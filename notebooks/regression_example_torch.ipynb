{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MILES-GUESS Regression Example Notebook\n",
    "\n",
    "John Schreck, David John Gagne, Charlie Becker, Gabrielle Gantos, Dhamma Kimpara, Thomas Martin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tqdm, yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "\n",
    "from mlguess.torch.models import DNN\n",
    "# from mlguess.keras.models import GaussianRegressorDNN, EvidentialRegressorDNN\n",
    "# from mlguess.keras.models import BaseRegressor as RegressorDNN\n",
    "# from mlguess.keras.callbacks import get_callbacks\n",
    "# from mlguess.regression_uq import compute_results\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config File\n",
    "\n",
    "#### Load the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/evidential_regression_torch.yml\"\n",
    "\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "#### Load Surface Layer data from the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/sample_cabauw_surface_layer.csv\")\n",
    "data[\"day\"] = data[\"Time\"].apply(lambda x: str(x).split(\" \")[0])\n",
    "data[\"year\"] = data[\"Time\"].apply(lambda x: str(x).split(\"-\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Valid-Test Splits\n",
    "\n",
    "This is a two-step process:\n",
    "1. Split all of the data on the day column between train (90%) and test (10%). The test data will be consisten accross all trained models and all data and model ensembles.\n",
    "2. Split the 90% training data from Step 1 into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Why do we need two seeds?\n",
    "data_seed = 0\n",
    "flat_seed = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need the same test_data for all trained models (data and model ensembles)\n",
    "gsp = GroupShuffleSplit(n_splits=1, random_state=flat_seed, train_size=0.9)\n",
    "splits = list(gsp.split(data, groups=data[\"year\"]))\n",
    "train_index, test_index = splits[0]\n",
    "train_data, test_data = data.iloc[train_index].copy(), data.iloc[test_index].copy() \n",
    "\n",
    "# Make N train-valid splits using day as grouping variable\n",
    "gsp = GroupShuffleSplit(n_splits=1,  random_state=flat_seed, train_size=0.885)\n",
    "splits = list(gsp.split(train_data, groups=train_data[\"year\"]))\n",
    "train_index, valid_index = splits[data_seed]\n",
    "train_data, valid_data = train_data.iloc[train_index].copy(), train_data.iloc[valid_index].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = conf[\"data\"][\"input_cols\"]\n",
    "#TODO: Should we include the other two output variables as potential options?\n",
    "output_cols = [\"friction_velocity:surface:m_s-1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler, y_scaler = RobustScaler(), MinMaxScaler((0, 1))\n",
    "x_train = x_scaler.fit_transform(train_data[input_cols])\n",
    "x_valid = x_scaler.transform(valid_data[input_cols])\n",
    "x_test = x_scaler.transform(test_data[input_cols])\n",
    "\n",
    "y_train = y_scaler.fit_transform(train_data[output_cols])\n",
    "y_valid = y_scaler.transform(valid_data[output_cols])\n",
    "y_test = y_scaler.transform(test_data[output_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Deterministic multi-layer perceptron (MLP) to predict some quantity\n",
    "\n",
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Why overwrite these variables in the config?\n",
    "conf[\"model\"][\"epochs\"] = 1\n",
    "conf[\"model\"][\"verbose\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN(\n",
    "    len(input_cols),\n",
    "    len(output_cols),\n",
    "    block_sizes = [1000], \n",
    "    dr = [0.5], \n",
    "    batch_norm = True, \n",
    "    lng = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a torch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurfaceLayerFluxDataset(Dataset):\n",
    "    def __init__(self, config, split='train'):\n",
    "        data_path = config['data_path']\n",
    "        input_cols = config['input_cols']\n",
    "        output_cols = config['output_cols']\n",
    "        flat_seed = config['split_params']['flat_seed']\n",
    "        data_seed = config['split_params']['data_seed']\n",
    "        train_size = config['split_ratios']['train_size']\n",
    "        valid_size = config['split_ratios']['valid_size']\n",
    "\n",
    "        # Load data\n",
    "        data = pd.read_csv(data_path)\n",
    "        data[\"day\"] = data[\"Time\"].apply(lambda x: str(x).split(\" \")[0])\n",
    "        data[\"year\"] = data[\"Time\"].apply(lambda x: str(x).split(\"-\")[0])\n",
    "\n",
    "        # Split data into train and test\n",
    "        gsp = GroupShuffleSplit(n_splits=1, random_state=flat_seed, train_size=train_size)\n",
    "        splits = list(gsp.split(data, groups=data[\"year\"]))\n",
    "        train_index, test_index = splits[0]\n",
    "        train_data, test_data = data.iloc[train_index].copy(), data.iloc[test_index].copy()\n",
    "\n",
    "        # Split train data into train and validation\n",
    "        gsp = GroupShuffleSplit(n_splits=1, random_state=flat_seed, train_size=valid_size)\n",
    "        splits = list(gsp.split(train_data, groups=train_data[\"year\"]))\n",
    "        train_index, valid_index = splits[data_seed]\n",
    "        train_data, valid_data = train_data.iloc[train_index].copy(), train_data.iloc[valid_index].copy()\n",
    "\n",
    "        # Initialize scalers\n",
    "        self.x_scaler = RobustScaler()\n",
    "        self.y_scaler = MinMaxScaler((0, 1))\n",
    "\n",
    "        # Fit scalers on training data\n",
    "        self.x_scaler.fit(train_data[input_cols])\n",
    "        self.y_scaler.fit(train_data[output_cols])\n",
    "\n",
    "        # Transform data\n",
    "        if split == 'train':\n",
    "            self.inputs = self.x_scaler.transform(train_data[input_cols])\n",
    "            self.targets = self.y_scaler.transform(train_data[output_cols])\n",
    "        elif split == 'valid':\n",
    "            self.inputs = self.x_scaler.transform(valid_data[input_cols])\n",
    "            self.targets = self.y_scaler.transform(valid_data[output_cols])\n",
    "        elif split == 'test':\n",
    "            self.inputs = self.x_scaler.transform(test_data[input_cols])\n",
    "            self.targets = self.y_scaler.transform(test_data[output_cols])\n",
    "        else:\n",
    "            raise ValueError(\"Invalid split value. Choose from 'train', 'valid', 'test'.\")\n",
    "\n",
    "        self.inputs = torch.tensor(self.inputs, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(self.targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.inputs[idx]\n",
    "        y = self.targets[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SurfaceLayerFluxDataset(conf['data'], split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RegressorDNN(**conf[\"model\"])\n",
    "# model.build_neural_network(x_train.shape[-1], y_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x_train,\n",
    "#           y_train,\n",
    "#           validation_data=(x_valid, y_valid),\n",
    "#           callbacks=get_callbacks(conf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = np.mean(np.abs(y_pred[:, 0]-test_data[output_cols[0]]))\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a Monte Carlo ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte_carlo_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict_monte_carlo(x_test, monte_carlo_steps, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_ensemble = np.mean(results, axis=0)\n",
    "var_ensemble = np.var(results, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Predict mu and sigma with a \"Gaussian MLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/surface_layer/gaussian.yml\"\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)\n",
    "\n",
    "conf[\"model\"][\"epochs\"] = 1\n",
    "conf[\"model\"][\"verbose\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_model = GaussianRegressorDNN(**conf[\"model\"])\n",
    "gauss_model.build_neural_network(x_train.shape[-1], y_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks=get_callbacks(conf)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, var = gauss_model.predict_uncertainty(x_test, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute variance and std from learned parameters\n",
    "#mu, var = gauss_model.calc_uncertainties(y_pred, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = np.mean(np.abs(mu[:, 0]-test_data[output_cols[0]]))\n",
    "print(mae, np.mean(var) ** (1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=test_data[output_cols[0]], y=mu[:, 0], kind='hex')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Predicted Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=mu[:, 0], y=np.sqrt(var)[:, 0], kind='hex')\n",
    "plt.xlabel('Predicted mu')\n",
    "plt.ylabel('Predicted sigma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute mu, aleatoric, and epistemic quantities using the evidential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/surface_layer/evidential.yml\"\n",
    "\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)\n",
    "\n",
    "conf[\"model\"][\"epochs\"] = 5\n",
    "conf[\"model\"][\"verbose\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_model = EvidentialRegressorDNN(**conf[\"model\"])\n",
    "ev_model.build_neural_network(x_train.shape[-1], y_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks=get_callbacks(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ev_model.predict_uncertainty(x_test, scaler=y_scaler)\n",
    "mu, aleatoric, epistemic = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = np.mean(np.abs(mu[:, 0] - test_data[output_cols[0]]))\n",
    "print(mae, np.mean(aleatoric)**(1/2), np.mean(epistemic)**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results(test_data,\n",
    "                output_cols,\n",
    "                mu,\n",
    "                np.sqrt(aleatoric),\n",
    "                np.sqrt(epistemic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create a deep ensemble with the Gaussian model so that the law of total variance can be applied to compute aleatoric and epistemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/surface_layer/gaussian.yml\"\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)\n",
    "\n",
    "conf[\"save_loc\"] = \"./\"\n",
    "conf[\"model\"][\"epochs\"] = 1\n",
    "conf[\"model\"][\"verbose\"] = 0\n",
    "n_splits = conf[\"ensemble\"][\"n_splits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make save directory for model weights\n",
    "os.makedirs(os.path.join(conf[\"save_loc\"], \"cv_ensemble\", \"models\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seed = 0\n",
    "gsp = GroupShuffleSplit(n_splits=1, random_state=flat_seed, train_size=0.9)\n",
    "splits = list(gsp.split(data, groups=data[\"day\"]))\n",
    "train_index, test_index = splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_mu = np.zeros((n_splits, test_data.shape[0], 1))\n",
    "ensemble_var = np.zeros((n_splits, test_data.shape[0], 1))\n",
    "\n",
    "for data_seed in tqdm.tqdm(range(n_splits)):\n",
    "    data = pd.read_csv(fn)\n",
    "    data[\"day\"] = data[\"Time\"].apply(lambda x: str(x).split(\" \")[0])\n",
    "\n",
    "    # Need the same test_data for all trained models (data and model ensembles)\n",
    "    flat_seed = 1000\n",
    "    gsp = GroupShuffleSplit(n_splits=1,\n",
    "                            random_state=flat_seed,\n",
    "                            train_size=0.9)\n",
    "    splits = list(gsp.split(data, groups=data[\"day\"]))\n",
    "    train_index, test_index = splits[0]\n",
    "    train_data, test_data = data.iloc[train_index].copy(), data.iloc[test_index].copy()\n",
    "\n",
    "    # Make N train-valid splits using day as grouping variable\n",
    "    gsp = GroupShuffleSplit(n_splits=n_splits, random_state=flat_seed, train_size=0.885)\n",
    "    splits = list(gsp.split(train_data, groups=train_data[\"day\"]))\n",
    "    train_index, valid_index = splits[data_seed]\n",
    "    train_data, valid_data = train_data.iloc[train_index].copy(), train_data.iloc[valid_index].copy()\n",
    "\n",
    "    x_scaler, y_scaler = RobustScaler(), MinMaxScaler((0, 1))\n",
    "    x_train = x_scaler.fit_transform(train_data[input_cols])\n",
    "    x_valid = x_scaler.transform(valid_data[input_cols])\n",
    "    x_test = x_scaler.transform(test_data[input_cols])\n",
    "\n",
    "    y_train = y_scaler.fit_transform(train_data[output_cols])\n",
    "    y_valid = y_scaler.transform(valid_data[output_cols])\n",
    "    y_test = y_scaler.transform(test_data[output_cols])\n",
    "\n",
    "    model = GaussianRegressorDNN(**conf[\"model\"])\n",
    "    model.build_neural_network(x_train.shape[-1], y_train.shape[-1])\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_valid, y_valid),\n",
    "        callbacks=get_callbacks(conf))\n",
    "\n",
    "    model.model_name = f\"cv_ensemble/models/model_seed0_split{data_seed}.h5\"\n",
    "    model.save_model()\n",
    "\n",
    "    # Save the best model\n",
    "    model.model_name = \"cv_ensemble/models/best.h5\"\n",
    "    model.save_model()\n",
    "\n",
    "    mu, var = model.predict_uncertainty(x_test, y_scaler)\n",
    "    mae = np.mean(np.abs(mu[:, 0]-test_data[output_cols[0]]))\n",
    "\n",
    "    ensemble_mu[data_seed] = mu\n",
    "    ensemble_var[data_seed] = var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use the method predict_ensemble to accomplish the same thing given pretrained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianRegressorDNN().load_model(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_mu, ensemble_var = model.predict_ensemble(x_test, scaler=y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epistemic = np.var(ensemble_mu, axis=0)\n",
    "aleatoric = np.mean(ensemble_var, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epistemic.mean()**(1/2), aleatoric.mean()**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results(test_data,\n",
    "                output_cols,\n",
    "                np.mean(ensemble_mu, axis=0),\n",
    "                np.sqrt(aleatoric),\n",
    "                np.sqrt(epistemic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Use Monte Carlo dropout with the Gaussian model to compute aleatoric and epistemic uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte_carlo_steps = 10\n",
    "\n",
    "ensemble_mu, ensemble_var = model.predict_monte_carlo(x_test,\n",
    "                                                      monte_carlo_steps,\n",
    "                                                      scaler=y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_epistemic = np.var(ensemble_mu, axis=0)\n",
    "ensemble_aleatoric = np.mean(ensemble_var, axis=0)\n",
    "ensemble_mean = np.mean(ensemble_mu, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evidential-casper",
   "language": "python",
   "name": "evidential-casper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
