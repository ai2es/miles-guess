{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MILES-GUESS Regression Example Notebook (PyTorch)\n",
    "\n",
    "John Schreck, David John Gagne, Charlie Becker, Gabrielle Gantos, Dhamma Kimpara, Thomas Martin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm \n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "\n",
    "from mlguess.torch.models import DNN\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config File\n",
    "\n",
    "#### Load the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/evidential_regression_torch.yml\"\n",
    "\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "#### Load Surface Layer data from the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/sample_cabauw_surface_layer.csv\")\n",
    "data[\"day\"] = data[\"Time\"].apply(lambda x: str(x).split(\" \")[0])\n",
    "data[\"year\"] = data[\"Time\"].apply(lambda x: str(x).split(\"-\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Valid-Test Splits\n",
    "\n",
    "This is a two-step process:\n",
    "1. Split all of the data on the day column between train (90%) and test (10%). The test data will be consisten accross all trained models and all data and model ensembles.\n",
    "2. Split the 90% training data from Step 1 into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Why do we need two seeds?\n",
    "data_seed = 0\n",
    "flat_seed = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need the same test_data for all trained models (data and model ensembles)\n",
    "gsp = GroupShuffleSplit(n_splits=1, random_state=flat_seed, train_size=0.9)\n",
    "splits = list(gsp.split(data, groups=data[\"year\"]))\n",
    "train_index, test_index = splits[0]\n",
    "train_data, test_data = data.iloc[train_index].copy(), data.iloc[test_index].copy() \n",
    "\n",
    "# Make N train-valid splits using day as grouping variable\n",
    "gsp = GroupShuffleSplit(n_splits=1,  random_state=flat_seed, train_size=0.885)\n",
    "splits = list(gsp.split(train_data, groups=train_data[\"year\"]))\n",
    "train_index, valid_index = splits[data_seed]\n",
    "train_data, valid_data = train_data.iloc[train_index].copy(), train_data.iloc[valid_index].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = conf[\"data\"][\"input_cols\"]\n",
    "#TODO: Should we include the other two output variables as potential options?\n",
    "output_cols = [\"friction_velocity:surface:m_s-1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler, y_scaler = RobustScaler(), MinMaxScaler((0, 1))\n",
    "x_train = x_scaler.fit_transform(train_data[input_cols])\n",
    "x_valid = x_scaler.transform(valid_data[input_cols])\n",
    "x_test = x_scaler.transform(test_data[input_cols])\n",
    "\n",
    "y_train = y_scaler.fit_transform(train_data[output_cols])\n",
    "y_valid = y_scaler.transform(valid_data[output_cols])\n",
    "y_test = y_scaler.transform(test_data[output_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(x_train)\n",
    "y = torch.FloatTensor(y_train)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Deterministic multi-layer perceptron (MLP) to predict some quantity\n",
    "\n",
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf[\"model\"][\"lng\"] = False\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN(**conf[\"model\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNN(\n",
       "  (fcn): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=1057, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.263, inplace=False)\n",
       "    (3): Linear(in_features=1057, out_features=1057, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): Dropout(p=0.263, inplace=False)\n",
       "    (6): Linear(in_features=1057, out_features=1057, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): Dropout(p=0.263, inplace=False)\n",
       "    (9): Linear(in_features=1057, out_features=1057, bias=True)\n",
       "    (10): LeakyReLU(negative_slope=0.01)\n",
       "    (11): Dropout(p=0.263, inplace=False)\n",
       "    (12): Linear(in_features=1057, out_features=1057, bias=True)\n",
       "    (13): LeakyReLU(negative_slope=0.01)\n",
       "    (14): Dropout(p=0.263, inplace=False)\n",
       "    (15): Linear(in_features=1057, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0258, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "results_dict = defaultdict(list)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "for i, (x, y) in enumerate(dataloader):\n",
    "    x = x.to(device)\n",
    "    y_pred = model(x)\n",
    "    y = y.to(device=device, dtype=x.dtype)\n",
    "    loss = criterion(y_pred, y.to(x.dtype)).mean()\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets use the evidential regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf[\"model\"][\"lng\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN(**conf[\"model\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNN(\n",
       "  (fcn): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=1057, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.263, inplace=False)\n",
       "    (3): Linear(in_features=1057, out_features=1057, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): Dropout(p=0.263, inplace=False)\n",
       "    (6): Linear(in_features=1057, out_features=1057, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): Dropout(p=0.263, inplace=False)\n",
       "    (9): Linear(in_features=1057, out_features=1057, bias=True)\n",
       "    (10): LeakyReLU(negative_slope=0.01)\n",
       "    (11): Dropout(p=0.263, inplace=False)\n",
       "    (12): Linear(in_features=1057, out_features=1057, bias=True)\n",
       "    (13): LeakyReLU(negative_slope=0.01)\n",
       "    (14): Dropout(p=0.263, inplace=False)\n",
       "    (15): LinearNormalGamma(\n",
       "      (linear): Linear(in_features=1057, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the training dataset variance to the model class to enable uncertainty calculations after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.training_var = [np.var(y_train)] # list of length 1 for 1 task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlguess.torch.regression_losses import LipschitzMSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.8424, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "results_dict = defaultdict(list)\n",
    "\n",
    "criterion = LipschitzMSELoss(**conf[\"train_loss\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "for i, (x, y) in enumerate(dataloader):\n",
    "    x = x.to(device)\n",
    "    y_pred = model(x)\n",
    "    gamma, nu, alpha, beta = y_pred\n",
    "    y = y.to(device=device, dtype=x.dtype)\n",
    "    loss = criterion(gamma, nu, alpha, beta, y.to(x.dtype))\n",
    "\n",
    "    # Predict uncertainties\n",
    "    y_pred = (_.cpu().detach() for _ in y_pred)\n",
    "    mu, ale, epi, total = model.predict_uncertainty(y_pred, y_scaler=y_scaler)\n",
    "    loss = loss.mean()\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions? Email John Schreck (schreck@ucar.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit",
   "language": "python",
   "name": "credit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
