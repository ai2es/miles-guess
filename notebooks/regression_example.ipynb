{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 14:32:35.010768: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-21 14:32:35.183614: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-21 14:32:35.969632: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /glade/work/schreck/miniconda3/envs/evidential/lib/python3.10/site-packages/nvidia/cudnn/lib:/glade/work/schreck/miniconda3/envs/evidential/lib/python3.10/site-packages/tensorrt_libs:/glade/work/schreck/miniconda3/envs/evidential/lib/:/glade/u/apps/dav/opt/cuda/11.4.0/extras/CUPTI/lib64:/glade/u/apps/dav/opt/cuda/11.4.0/lib64:/glade/u/apps/dav/opt/openmpi/4.1.1/intel/19.1.1/lib:/glade/u/apps/dav/opt/ucx/1.11.0/lib:/glade/u/apps/opt/intel/2020u1/compilers_and_libraries/linux/lib/intel64\n",
      "2023-09-21 14:32:35.969762: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /glade/work/schreck/miniconda3/envs/evidential/lib/python3.10/site-packages/nvidia/cudnn/lib:/glade/work/schreck/miniconda3/envs/evidential/lib/python3.10/site-packages/tensorrt_libs:/glade/work/schreck/miniconda3/envs/evidential/lib/:/glade/u/apps/dav/opt/cuda/11.4.0/extras/CUPTI/lib64:/glade/u/apps/dav/opt/cuda/11.4.0/lib64:/glade/u/apps/dav/opt/openmpi/4.1.1/intel/19.1.1/lib:/glade/u/apps/dav/opt/ucx/1.11.0/lib:/glade/u/apps/opt/intel/2020u1/compilers_and_libraries/linux/lib/intel64\n",
      "2023-09-21 14:32:35.969772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gc, tqdm, copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a configuation yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/surface_layer/mlp.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load example training data (the full data set is available on Zenodo ... ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"../data/sample_cabauw_surface_layer.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = conf[\"data\"][\"input_cols\"]\n",
    "output_cols = ['friction_velocity:surface:m_s-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[[\"Time\"]+input_cols+output_cols].sample(frac=0.1).to_csv(\"../data/sample_cabauw_surface_layer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seed = 0\n",
    "flat_seed = 1000\n",
    "\n",
    "n_splits = 1\n",
    "n_models = 1\n",
    "\n",
    "data = pd.read_csv(fn)\n",
    "data[\"day\"] = data[\"Time\"].apply(lambda x: str(x).split(\" \")[0])\n",
    "\n",
    "# Need the same test_data for all trained models (data and model ensembles)\n",
    "flat_seed = 1000\n",
    "gsp = GroupShuffleSplit(n_splits=1,  random_state = flat_seed, train_size=0.9)\n",
    "splits = list(gsp.split(data, groups = data[\"day\"]))\n",
    "train_index, test_index = splits[0]\n",
    "train_data, test_data = data.iloc[train_index].copy(), data.iloc[test_index].copy() \n",
    "\n",
    "# Make N train-valid splits using day as grouping variable\n",
    "gsp = GroupShuffleSplit(n_splits=n_splits,  random_state = flat_seed, train_size=0.885)\n",
    "splits = list(gsp.split(train_data, groups = train_data[\"day\"]))\n",
    "train_index, valid_index = splits[data_seed]\n",
    "train_data, valid_data = train_data.iloc[train_index].copy(), train_data.iloc[valid_index] .copy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler, y_scaler = RobustScaler(), MinMaxScaler((0, 1))\n",
    "x_train = x_scaler.fit_transform(train_data[input_cols])\n",
    "x_valid = x_scaler.transform(valid_data[input_cols])\n",
    "x_test = x_scaler.transform(test_data[input_cols])\n",
    "\n",
    "y_train = y_scaler.fit_transform(train_data[output_cols])\n",
    "y_valid = y_scaler.transform(valid_data[output_cols])\n",
    "y_test = y_scaler.transform(test_data[output_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Deterministic multi-layer perceptron (MLP) to predict some quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (models.py, line 376)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/glade/work/schreck/miniconda3/envs/evidential/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3505\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\n\u001b[0;31m    from evml.keras.models import BaseRegressor as RegressorDNN\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m/glade/work/schreck/miniconda3/envs/evidential/lib/python3.8/site-packages/evml/keras/models.py:376\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.ensemble_weights[0].replace\".h5\", \"_training_var.txt\"\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from evml.keras.models import BaseRegressor as RegressorDNN\n",
    "#from evml.keras.models import RegressorDNN\n",
    "from evml.keras.callbacks import get_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf[\"model\"][\"epochs\"] = 1\n",
    "conf[\"model\"][\"verbose\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressorDNN(**conf[\"model\"])\n",
    "model.build_neural_network(x_train.shape[-1], y_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks=get_callbacks(conf, path_extend=f\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = np.mean(np.abs(y_pred[:, 0]-test_data[output_cols[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a Monte Carlo ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte_carlo_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict_monte_carlo(x_test, y_test, monte_carlo_steps, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_ensemble = np.mean(results, axis = 0)\n",
    "var_ensemble = np.var(results, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_ensemble.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_ensemble.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Predict mu and sigma with a \"Gaussian MLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evml.keras.models import GaussianRegressorDNN\n",
    "#from evml.keras.models import GaussianRegressorDNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/surface_layer/gaussian.yml\"\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)\n",
    "    \n",
    "conf[\"model\"][\"epochs\"] = 1\n",
    "conf[\"model\"][\"verbose\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_model = GaussianRegressorDNN(**conf[\"model\"])\n",
    "gauss_model.build_neural_network(x_train.shape[-1], y_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks=get_callbacks(conf, path_extend=f\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, var = gauss_model.predict_uncertainty(x_test, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute variance and std from learned parameters\n",
    "#mu, var = gauss_model.calc_uncertainties(y_pred, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = np.mean(np.abs(mu[:, 0]-test_data[output_cols[0]]))\n",
    "print(mae, np.mean(var) ** (1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.jointplot(x = test_data[output_cols[0]], y = mu[:, 0], kind = 'hex')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Predicted Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x = mu[:, 0], y = np.sqrt(var)[:, 0], kind = 'hex')\n",
    "plt.xlabel('Predicted mu')\n",
    "plt.ylabel('Predicted sigma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute mu, aleatoric, and epistemic quantities using the evidential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evml.keras.models import EvidentialRegressorDNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/surface_layer/evidential.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)\n",
    "    \n",
    "conf[\"model\"][\"epochs\"] = 5\n",
    "conf[\"model\"][\"verbose\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_model = EvidentialRegressorDNN(**conf[\"model\"])\n",
    "ev_model.build_neural_network(x_train.shape[-1], y_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks=get_callbacks(conf, path_extend=f\"\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ev_model.predict_uncertainty(x_test, scaler=y_scaler)\n",
    "mu, aleatoric, epistemic = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = np.mean(np.abs(mu[:, 0]-test_data[output_cols[0]]))\n",
    "print(mae, np.mean(aleatoric) ** (1/2), np.mean(epistemic) ** (1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evml.regression_uq import compute_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results(test_data, output_cols, mu, np.sqrt(aleatoric), np.sqrt(epistemic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create a deep ensemble with the Gaussian model so that the law of total variance can be applied to compute aleatoric and epistemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/surface_layer/gaussian.yml\"\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)\n",
    "    \n",
    "conf[\"save_loc\"] = \"./\"\n",
    "conf[\"model\"][\"epochs\"] = 1\n",
    "conf[\"model\"][\"verbose\"] = 0\n",
    "n_splits = conf[\"ensemble\"][\"n_splits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make save directory for model weights\n",
    "os.makedirs(os.path.join(conf[\"save_loc\"], \"cv_ensemble\", \"models\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seed = 0\n",
    "gsp = GroupShuffleSplit(n_splits=1,  random_state = flat_seed, train_size=0.9)\n",
    "splits = list(gsp.split(data, groups = data[\"day\"]))\n",
    "train_index, test_index = splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_mu = np.zeros((n_splits, test_data.shape[0], 1))\n",
    "ensemble_var = np.zeros((n_splits, test_data.shape[0], 1))\n",
    "\n",
    "for data_seed in tqdm.tqdm(range(n_splits)):\n",
    "    data = pd.read_csv(fn)\n",
    "    data[\"day\"] = data[\"Time\"].apply(lambda x: str(x).split(\" \")[0])\n",
    "\n",
    "    # Need the same test_data for all trained models (data and model ensembles)\n",
    "    flat_seed = 1000\n",
    "    gsp = GroupShuffleSplit(n_splits=1,  random_state = flat_seed, train_size=0.9)\n",
    "    splits = list(gsp.split(data, groups = data[\"day\"]))\n",
    "    train_index, test_index = splits[0]\n",
    "    train_data, test_data = data.iloc[train_index].copy(), data.iloc[test_index].copy() \n",
    "\n",
    "    # Make N train-valid splits using day as grouping variable\n",
    "    gsp = GroupShuffleSplit(n_splits=n_splits,  random_state = flat_seed, train_size=0.885)\n",
    "    splits = list(gsp.split(train_data, groups = train_data[\"day\"]))\n",
    "    train_index, valid_index = splits[data_seed]\n",
    "    train_data, valid_data = train_data.iloc[train_index].copy(), train_data.iloc[valid_index] .copy()  \n",
    "\n",
    "    x_scaler, y_scaler = RobustScaler(), MinMaxScaler((0, 1))\n",
    "    x_train = x_scaler.fit_transform(train_data[input_cols])\n",
    "    x_valid = x_scaler.transform(valid_data[input_cols])\n",
    "    x_test = x_scaler.transform(test_data[input_cols])\n",
    "\n",
    "    y_train = y_scaler.fit_transform(train_data[output_cols])\n",
    "    y_valid = y_scaler.transform(valid_data[output_cols])\n",
    "    y_test = y_scaler.transform(test_data[output_cols])\n",
    "    \n",
    "    model = GaussianRegressorDNN(**conf[\"model\"])\n",
    "    model.build_neural_network(x_train.shape[-1], y_train.shape[-1])\n",
    "    \n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_valid, y_valid),\n",
    "        callbacks=get_callbacks(conf, path_extend=f\"\")\n",
    "    )\n",
    "    \n",
    "    model.model_name = f\"cv_ensemble/models/model_seed0_split{data_seed}.h5\"\n",
    "    model.save_model()\n",
    "    \n",
    "    # Save the best model \n",
    "    model.model_name = f\"cv_ensemble/models/best.h5\"\n",
    "    model.save_model()\n",
    "    \n",
    "    mu, var = model.predict_uncertainty(x_test, y_scaler)\n",
    "    mae = np.mean(np.abs(mu[:, 0]-test_data[output_cols[0]]))\n",
    "    \n",
    "    ensemble_mu[data_seed] = mu\n",
    "    ensemble_var[data_seed] = var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use the method predict_ensemble to accomplish the same thing given pretrained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianRegressorDNN().load_model(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ensemble_weights[0].replace(\".h5\", \"_training_var.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.loadtxt(model.ensemble_weights[0].strip(\".h5\") + \"_training_var.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = [f\"./model_split{data_seed}.h5\" for data_seed in range(n_splits)]\n",
    "ensemble_mu, ensemble_var = model.predict_ensemble(x_test, scaler = y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epistemic = np.var(ensemble_mu, axis=0)\n",
    "aleatoric = np.mean(ensemble_var, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epistemic.mean() ** (1/2), aleatoric.mean() ** (1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results(test_data, output_cols, np.mean(ensemble_mu, axis=0), np.sqrt(aleatoric), np.sqrt(epistemic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Use Monte Carlo dropout with the Gaussian model to compute aleatoric and epistemic uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte_carlo_steps = 10\n",
    "\n",
    "ensemble_mu, ensemble_var = model.predict_monte_carlo(x_test, y_test, monte_carlo_steps, scaler = y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_epistemic = np.var(ensemble_mu, axis=0)\n",
    "ensemble_aleatoric = np.mean(ensemble_var, axis=0)\n",
    "ensemble_mean = np.mean(ensemble_mu, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-evidential]",
   "language": "python",
   "name": "conda-env-miniconda3-evidential-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
