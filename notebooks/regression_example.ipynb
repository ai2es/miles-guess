{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MILES-GUESS Regression Example Notebook\n",
    "\n",
    "John Schreck, David John Gagne, Charlie Becker, Gabrielle Gantos, Dhamma Kimpara, Thomas Martin\n",
    "- distinguish between authors and contributors\n",
    "- add orchids for people?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "\n",
    "This notebook is meant to demonstrate a functional example of how to use the miles-guess repository for training an evidential regression model.\n",
    "\n",
    "Steps to get this notebook running:\n",
    "1) Follow package installation steps in the miles-guess [ReadMe](https://github.com/ai2es/miles-guess/tree/main).\n",
    "2) Run the cells in this notebook in order.\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "* Evidential deep learning is capable of training a single model to simultaneously learn the problem and estimate the problem's aleatoric and epistemic uncertainties. See this paper here().\n",
    "* This notebook represents an example of evidential deep learning for a regression problem. Specifically, that regression problem\n",
    "* Data? (Make sure to mention the subsetting of the dataset for ptype)\n",
    "* Parameters?\n",
    "* What will user be prepared to do with miles-guess after running this notebook?\n",
    "\n",
    "\n",
    "#### Notes I can add\n",
    "* Here is where data comes in\n",
    "* Here's where you can vary a parameter or not\n",
    "* Defaults to parameters\n",
    "* Limits to the parameters (only good for this timestep, only good for this seed, etc)\n",
    "* Physical meaning to these parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tqdm, yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "\n",
    "from mlguess.keras.models import GaussianRegressorDNN, EvidentialRegressorDNN\n",
    "from mlguess.keras.models import BaseRegressor as RegressorDNN\n",
    "from mlguess.keras.callbacks import get_callbacks\n",
    "from mlguess.regression_uq import compute_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config File\n",
    "\n",
    "#### Load the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/surface_layer/mlp.yml\"\n",
    "\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "#### Load Surface Layer data from the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/sample_cabauw_surface_layer.csv\")\n",
    "data[\"day\"] = data[\"Time\"].apply(lambda x: str(x).split(\" \")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Valid-Test Splits\n",
    "\n",
    "This is a two-step process:\n",
    "1. Split all of the data on the day column between train (90%) and test (10%). The test data will be consisten accross all trained models and all data and model ensembles.\n",
    "2. Split the 90% training data from Step 1 into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Why do we need two seeds?\n",
    "data_seed = 0\n",
    "flat_seed = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need the same test_data for all trained models (data and model ensembles)\n",
    "gsp = GroupShuffleSplit(n_splits=1, random_state=flat_seed, train_size=0.9)\n",
    "splits = list(gsp.split(data, groups=data[\"day\"]))\n",
    "train_index, test_index = splits[0]\n",
    "train_data, test_data = data.iloc[train_index].copy(), data.iloc[test_index].copy() \n",
    "\n",
    "# Make N train-valid splits using day as grouping variable\n",
    "gsp = GroupShuffleSplit(n_splits=1,  random_state=flat_seed, train_size=0.885)\n",
    "splits = list(gsp.split(train_data, groups=train_data[\"day\"]))\n",
    "train_index, valid_index = splits[data_seed]\n",
    "train_data, valid_data = train_data.iloc[train_index].copy(), train_data.iloc[valid_index].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = conf[\"data\"][\"input_cols\"]\n",
    "#TODO: Should we include the other two output variables as potential options?\n",
    "output_cols = [\"friction_velocity:surface:m_s-1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler, y_scaler = RobustScaler(), MinMaxScaler((0, 1))\n",
    "x_train = x_scaler.fit_transform(train_data[input_cols])\n",
    "x_valid = x_scaler.transform(valid_data[input_cols])\n",
    "x_test = x_scaler.transform(test_data[input_cols])\n",
    "\n",
    "y_train = y_scaler.fit_transform(train_data[output_cols])\n",
    "y_valid = y_scaler.transform(valid_data[output_cols])\n",
    "y_test = y_scaler.transform(test_data[output_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Deterministic multi-layer perceptron (MLP) to predict some quantity\n",
    "\n",
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Why overwrite these variables in the config?\n",
    "conf[\"model\"][\"epochs\"] = 1\n",
    "conf[\"model\"][\"verbose\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressorDNN(**conf[\"model\"])\n",
    "model.build_neural_network(x_train.shape[-1], y_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          validation_data=(x_valid, y_valid),\n",
    "          callbacks=get_callbacks(conf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = np.mean(np.abs(y_pred[:, 0]-test_data[output_cols[0]]))\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a Monte Carlo ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte_carlo_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict_monte_carlo(x_test, monte_carlo_steps, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_ensemble = np.mean(results, axis=0)\n",
    "var_ensemble = np.var(results, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Predict mu and sigma with a \"Gaussian MLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/surface_layer/gaussian.yml\"\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)\n",
    "\n",
    "conf[\"model\"][\"epochs\"] = 1\n",
    "conf[\"model\"][\"verbose\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_model = GaussianRegressorDNN(**conf[\"model\"])\n",
    "gauss_model.build_neural_network(x_train.shape[-1], y_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks=get_callbacks(conf)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, var = gauss_model.predict_uncertainty(x_test, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute variance and std from learned parameters\n",
    "#mu, var = gauss_model.calc_uncertainties(y_pred, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = np.mean(np.abs(mu[:, 0]-test_data[output_cols[0]]))\n",
    "print(mae, np.mean(var) ** (1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=test_data[output_cols[0]], y=mu[:, 0], kind='hex')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Predicted Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=mu[:, 0], y=np.sqrt(var)[:, 0], kind='hex')\n",
    "plt.xlabel('Predicted mu')\n",
    "plt.ylabel('Predicted sigma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute mu, aleatoric, and epistemic quantities using the evidential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/surface_layer/evidential.yml\"\n",
    "\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)\n",
    "\n",
    "conf[\"model\"][\"epochs\"] = 5\n",
    "conf[\"model\"][\"verbose\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_model = EvidentialRegressorDNN(**conf[\"model\"])\n",
    "ev_model.build_neural_network(x_train.shape[-1], y_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks=get_callbacks(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ev_model.predict_uncertainty(x_test, scaler=y_scaler)\n",
    "mu, aleatoric, epistemic = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = np.mean(np.abs(mu[:, 0] - test_data[output_cols[0]]))\n",
    "print(mae, np.mean(aleatoric)**(1/2), np.mean(epistemic)**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results(test_data,\n",
    "                output_cols,\n",
    "                mu,\n",
    "                np.sqrt(aleatoric),\n",
    "                np.sqrt(epistemic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create a deep ensemble with the Gaussian model so that the law of total variance can be applied to compute aleatoric and epistemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/surface_layer/gaussian.yml\"\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)\n",
    "\n",
    "conf[\"save_loc\"] = \"./\"\n",
    "conf[\"model\"][\"epochs\"] = 1\n",
    "conf[\"model\"][\"verbose\"] = 0\n",
    "n_splits = conf[\"ensemble\"][\"n_splits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make save directory for model weights\n",
    "os.makedirs(os.path.join(conf[\"save_loc\"], \"cv_ensemble\", \"models\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seed = 0\n",
    "gsp = GroupShuffleSplit(n_splits=1, random_state=flat_seed, train_size=0.9)\n",
    "splits = list(gsp.split(data, groups=data[\"day\"]))\n",
    "train_index, test_index = splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_mu = np.zeros((n_splits, test_data.shape[0], 1))\n",
    "ensemble_var = np.zeros((n_splits, test_data.shape[0], 1))\n",
    "\n",
    "for data_seed in tqdm.tqdm(range(n_splits)):\n",
    "    data = pd.read_csv(fn)\n",
    "    data[\"day\"] = data[\"Time\"].apply(lambda x: str(x).split(\" \")[0])\n",
    "\n",
    "    # Need the same test_data for all trained models (data and model ensembles)\n",
    "    flat_seed = 1000\n",
    "    gsp = GroupShuffleSplit(n_splits=1,\n",
    "                            random_state=flat_seed,\n",
    "                            train_size=0.9)\n",
    "    splits = list(gsp.split(data, groups=data[\"day\"]))\n",
    "    train_index, test_index = splits[0]\n",
    "    train_data, test_data = data.iloc[train_index].copy(), data.iloc[test_index].copy()\n",
    "\n",
    "    # Make N train-valid splits using day as grouping variable\n",
    "    gsp = GroupShuffleSplit(n_splits=n_splits, random_state=flat_seed, train_size=0.885)\n",
    "    splits = list(gsp.split(train_data, groups=train_data[\"day\"]))\n",
    "    train_index, valid_index = splits[data_seed]\n",
    "    train_data, valid_data = train_data.iloc[train_index].copy(), train_data.iloc[valid_index].copy()\n",
    "\n",
    "    x_scaler, y_scaler = RobustScaler(), MinMaxScaler((0, 1))\n",
    "    x_train = x_scaler.fit_transform(train_data[input_cols])\n",
    "    x_valid = x_scaler.transform(valid_data[input_cols])\n",
    "    x_test = x_scaler.transform(test_data[input_cols])\n",
    "\n",
    "    y_train = y_scaler.fit_transform(train_data[output_cols])\n",
    "    y_valid = y_scaler.transform(valid_data[output_cols])\n",
    "    y_test = y_scaler.transform(test_data[output_cols])\n",
    "\n",
    "    model = GaussianRegressorDNN(**conf[\"model\"])\n",
    "    model.build_neural_network(x_train.shape[-1], y_train.shape[-1])\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_valid, y_valid),\n",
    "        callbacks=get_callbacks(conf))\n",
    "\n",
    "    model.model_name = f\"cv_ensemble/models/model_seed0_split{data_seed}.h5\"\n",
    "    model.save_model()\n",
    "\n",
    "    # Save the best model\n",
    "    model.model_name = \"cv_ensemble/models/best.h5\"\n",
    "    model.save_model()\n",
    "\n",
    "    mu, var = model.predict_uncertainty(x_test, y_scaler)\n",
    "    mae = np.mean(np.abs(mu[:, 0]-test_data[output_cols[0]]))\n",
    "\n",
    "    ensemble_mu[data_seed] = mu\n",
    "    ensemble_var[data_seed] = var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use the method predict_ensemble to accomplish the same thing given pretrained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianRegressorDNN().load_model(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_mu, ensemble_var = model.predict_ensemble(x_test, scaler=y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epistemic = np.var(ensemble_mu, axis=0)\n",
    "aleatoric = np.mean(ensemble_var, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epistemic.mean()**(1/2), aleatoric.mean()**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results(test_data,\n",
    "                output_cols,\n",
    "                np.mean(ensemble_mu, axis=0),\n",
    "                np.sqrt(aleatoric),\n",
    "                np.sqrt(epistemic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Use Monte Carlo dropout with the Gaussian model to compute aleatoric and epistemic uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte_carlo_steps = 10\n",
    "\n",
    "ensemble_mu, ensemble_var = model.predict_monte_carlo(x_test,\n",
    "                                                      monte_carlo_steps,\n",
    "                                                      scaler=y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_epistemic = np.var(ensemble_mu, axis=0)\n",
    "ensemble_aleatoric = np.mean(ensemble_var, axis=0)\n",
    "ensemble_mean = np.mean(ensemble_mu, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:guess]",
   "language": "python",
   "name": "conda-env-guess-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
