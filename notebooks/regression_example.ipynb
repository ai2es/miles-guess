{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MILES-GUESS Regression Example Notebook\n",
    "\n",
    "John Schreck, David John Gagne, Charlie Becker, Gabrielle Gantos, Dhamma Kimpara, Thomas Martin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "\n",
    "This notebook is meant to demonstrate a functional example of how to use the miles-guess repository for training an evidential regression model.\n",
    "\n",
    "Steps to get this notebook running:\n",
    "1) Follow package installation steps in the miles-guess [ReadMe](https://github.com/ai2es/miles-guess/tree/main).\n",
    "2) Run the cells in this notebook in order.\n",
    "\n",
    "#### Quick Intro to Surface Layer (SL)\n",
    "\n",
    "The Surface Layer (SL) problem attempts to predict energy fluxes from Earth's surface given near-surface atmospheric conditions. The dataset used in this example problem originates from the Royal Netherlands Meteorological Institute (KNMI) Cabauw Experimental Site for Atmospheric Research. The outgoing longwave radiation, converted to skin temperature, is derived from a device located approximately 200 meters from the flux tower.\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "* Evidential deep learning is capable of training a single model to simultaneously learn the problem and estimate the problem's aleatoric and epistemic uncertainties. See this paper here().\n",
    "* This notebook represents an example of evidential deep learning for a regression problem. Specifically, that regression problem\n",
    "* Data? (Make sure to mention the subsetting of the dataset for ptype)\n",
    "* What will user be prepared to do with miles-guess after running this notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 17:54:32.060087: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-12 17:54:32.110346: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-12 17:54:32.949343: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os, tqdm, yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "\n",
    "from mlguess.keras.models import EvidentialRegressorDNN\n",
    "from mlguess.keras.callbacks import get_callbacks\n",
    "from mlguess.regression_uq import compute_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config File\n",
    "\n",
    "#### Parameters of interest\n",
    "\n",
    "#TODO: Highlight parameters of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/surface_layer/mlp.yml\"\n",
    "\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/sample_cabauw_surface_layer.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Valid-Test Splits\n",
    "\n",
    "This is a three-step process:\n",
    "1. Group data by day.\n",
    "2. Split all of the data on the day column between train (90%) and test (10%). The test data will be consisten accross all trained models and all data and model ensembles.\n",
    "3. Split the 90% training data from Step 1 into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"day\"] = data[\"Time\"].apply(lambda x: str(x).split(\" \")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seed = 0\n",
    "flat_seed = 1000\n",
    "\n",
    "# Need the same test_data for all trained models (data and model ensembles)\n",
    "gsp = GroupShuffleSplit(n_splits=1, random_state=flat_seed, train_size=0.9)\n",
    "splits = list(gsp.split(data, groups=data[\"day\"]))\n",
    "train_index, test_index = splits[0]\n",
    "train_data, test_data = data.iloc[train_index].copy(), data.iloc[test_index].copy() \n",
    "\n",
    "# Make N train-valid splits using day as grouping variable\n",
    "gsp = GroupShuffleSplit(n_splits=1,  random_state=flat_seed, train_size=0.885)\n",
    "splits = list(gsp.split(train_data, groups=train_data[\"day\"]))\n",
    "train_index, valid_index = splits[data_seed]\n",
    "train_data, valid_data = train_data.iloc[train_index].copy(), train_data.iloc[valid_index].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = conf[\"data\"][\"input_cols\"]\n",
    "#TODO: Should we include the other two output variables as potential options?\n",
    "output_cols = [\"friction_velocity:surface:m_s-1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler, y_scaler = RobustScaler(), MinMaxScaler((0, 1))\n",
    "x_train = x_scaler.fit_transform(train_data[input_cols])\n",
    "x_valid = x_scaler.transform(valid_data[input_cols])\n",
    "x_test = x_scaler.transform(test_data[input_cols])\n",
    "\n",
    "y_train = y_scaler.fit_transform(train_data[output_cols])\n",
    "y_valid = y_scaler.transform(valid_data[output_cols])\n",
    "y_test = y_scaler.transform(test_data[output_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Deterministic multi-layer perceptron (MLP) to predict some quantity\n",
    "\n",
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Why overwrite these variables in the config?\n",
    "conf[\"model\"][\"epochs\"] = 1\n",
    "conf[\"model\"][\"verbose\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 17:54:35.228747: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "model = EvidentialRegressorDNN(**conf[\"model\"])\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  86/1795\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 588us/step - loss: 3.0557e-05  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ggantos/conda-envs/guess/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:576: UserWarning: Gradients do not exist for variables ['bias', 'kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1795/1795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 669us/step - loss: 2.9939e-05 - val_loss: 2.8053e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1460bc91e200>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test, return_uncertainties=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06768488,  0.61546266,  1.7284483 ,  0.65492064],\n",
       "       [-0.02590589,  0.7062391 ,  1.6261563 ,  0.66753787],\n",
       "       [ 0.12891376,  0.7776666 ,  1.5624065 ,  0.73487395],\n",
       "       ...,\n",
       "       [-0.01874506,  0.7131814 ,  1.6728607 ,  0.7000923 ],\n",
       "       [ 0.00279877,  0.73572874,  1.6917179 ,  0.7401652 ],\n",
       "       [-0.07375085,  0.6940977 ,  1.7539929 ,  0.6484979 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2545100680072519"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = np.mean(np.abs(y_pred[:, 0] - test_data[output_cols[0]]))\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Monte Carlo ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "ensemble:\n",
    "    n_models: 1\n",
    "    n_splits: 1\n",
    "    monte_carlo_passes: 100\n",
    "```\n",
    "Create an ensemble of models (random initialization) using cross-validation splits. If MC passes > 0, an ensemble is created after each model finishes training on the test holdout. The LOTV may then be applied to the ensemble created from cross-validation. Otherwise a single ensemble is created but the LOTV is not applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EvidentialRegressorDNN' object has no attribute 'predict_monte_carlo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_monte_carlo\u001b[49m(x_test,\n\u001b[1;32m      2\u001b[0m                                     conf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonte_carlo_passes\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      3\u001b[0m                                     y_scaler)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EvidentialRegressorDNN' object has no attribute 'predict_monte_carlo'"
     ]
    }
   ],
   "source": [
    "# TODO: will have a separate ensemble class implemented in the future\n",
    "results = model.predict_monte_carlo(x_test,\n",
    "                                    conf[\"monte_carlo_passes\"],\n",
    "                                    y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_ensemble = np.mean(results, axis=0)\n",
    "var_ensemble = np.var(results, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Predict mu and sigma with a \"Gaussian MLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/surface_layer/gaussian.yml\"\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)\n",
    "\n",
    "conf[\"model\"][\"epochs\"] = 1\n",
    "conf[\"model\"][\"verbose\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_model = GaussianRegressorDNN(**conf[\"model\"])\n",
    "gauss_model.build_neural_network(x_train.shape[-1], y_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks=get_callbacks(conf)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, var = gauss_model.predict_uncertainty(x_test, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute variance and std from learned parameters\n",
    "#mu, var = gauss_model.calc_uncertainties(y_pred, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = np.mean(np.abs(mu[:, 0]-test_data[output_cols[0]]))\n",
    "print(mae, np.mean(var) ** (1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=test_data[output_cols[0]], y=mu[:, 0], kind='hex')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Predicted Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=mu[:, 0], y=np.sqrt(var)[:, 0], kind='hex')\n",
    "plt.xlabel('Predicted mu')\n",
    "plt.ylabel('Predicted sigma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute mu, aleatoric, and epistemic quantities using the evidential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/surface_layer/evidential.yml\"\n",
    "\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)\n",
    "\n",
    "conf[\"model\"][\"epochs\"] = 5\n",
    "conf[\"model\"][\"verbose\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_model = EvidentialRegressorDNN(**conf[\"model\"])\n",
    "ev_model.build_neural_network(x_train.shape[-1], y_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks=get_callbacks(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ev_model.predict_uncertainty(x_test, scaler=y_scaler)\n",
    "mu, aleatoric, epistemic = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = np.mean(np.abs(mu[:, 0] - test_data[output_cols[0]]))\n",
    "print(mae, np.mean(aleatoric)**(1/2), np.mean(epistemic)**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results(test_data,\n",
    "                output_cols,\n",
    "                mu,\n",
    "                np.sqrt(aleatoric),\n",
    "                np.sqrt(epistemic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create a deep ensemble with the Gaussian model so that the law of total variance can be applied to compute aleatoric and epistemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/surface_layer/gaussian.yml\"\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)\n",
    "\n",
    "conf[\"save_loc\"] = \"./\"\n",
    "conf[\"model\"][\"epochs\"] = 1\n",
    "conf[\"model\"][\"verbose\"] = 0\n",
    "n_splits = conf[\"ensemble\"][\"n_splits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make save directory for model weights\n",
    "os.makedirs(os.path.join(conf[\"save_loc\"], \"cv_ensemble\", \"models\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seed = 0\n",
    "gsp = GroupShuffleSplit(n_splits=1, random_state=flat_seed, train_size=0.9)\n",
    "splits = list(gsp.split(data, groups=data[\"day\"]))\n",
    "train_index, test_index = splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_mu = np.zeros((n_splits, test_data.shape[0], 1))\n",
    "ensemble_var = np.zeros((n_splits, test_data.shape[0], 1))\n",
    "\n",
    "for data_seed in tqdm.tqdm(range(n_splits)):\n",
    "    data = pd.read_csv(fn)\n",
    "    data[\"day\"] = data[\"Time\"].apply(lambda x: str(x).split(\" \")[0])\n",
    "\n",
    "    # Need the same test_data for all trained models (data and model ensembles)\n",
    "    flat_seed = 1000\n",
    "    gsp = GroupShuffleSplit(n_splits=1,\n",
    "                            random_state=flat_seed,\n",
    "                            train_size=0.9)\n",
    "    splits = list(gsp.split(data, groups=data[\"day\"]))\n",
    "    train_index, test_index = splits[0]\n",
    "    train_data, test_data = data.iloc[train_index].copy(), data.iloc[test_index].copy()\n",
    "\n",
    "    # Make N train-valid splits using day as grouping variable\n",
    "    gsp = GroupShuffleSplit(n_splits=n_splits, random_state=flat_seed, train_size=0.885)\n",
    "    splits = list(gsp.split(train_data, groups=train_data[\"day\"]))\n",
    "    train_index, valid_index = splits[data_seed]\n",
    "    train_data, valid_data = train_data.iloc[train_index].copy(), train_data.iloc[valid_index].copy()\n",
    "\n",
    "    x_scaler, y_scaler = RobustScaler(), MinMaxScaler((0, 1))\n",
    "    x_train = x_scaler.fit_transform(train_data[input_cols])\n",
    "    x_valid = x_scaler.transform(valid_data[input_cols])\n",
    "    x_test = x_scaler.transform(test_data[input_cols])\n",
    "\n",
    "    y_train = y_scaler.fit_transform(train_data[output_cols])\n",
    "    y_valid = y_scaler.transform(valid_data[output_cols])\n",
    "    y_test = y_scaler.transform(test_data[output_cols])\n",
    "\n",
    "    model = GaussianRegressorDNN(**conf[\"model\"])\n",
    "    model.build_neural_network(x_train.shape[-1], y_train.shape[-1])\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_valid, y_valid),\n",
    "        callbacks=get_callbacks(conf))\n",
    "\n",
    "    model.model_name = f\"cv_ensemble/models/model_seed0_split{data_seed}.h5\"\n",
    "    model.save_model()\n",
    "\n",
    "    # Save the best model\n",
    "    model.model_name = \"cv_ensemble/models/best.h5\"\n",
    "    model.save_model()\n",
    "\n",
    "    mu, var = model.predict_uncertainty(x_test, y_scaler)\n",
    "    mae = np.mean(np.abs(mu[:, 0]-test_data[output_cols[0]]))\n",
    "\n",
    "    ensemble_mu[data_seed] = mu\n",
    "    ensemble_var[data_seed] = var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use the method predict_ensemble to accomplish the same thing given pretrained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianRegressorDNN().load_model(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_mu, ensemble_var = model.predict_ensemble(x_test, scaler=y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epistemic = np.var(ensemble_mu, axis=0)\n",
    "aleatoric = np.mean(ensemble_var, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epistemic.mean()**(1/2), aleatoric.mean()**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results(test_data,\n",
    "                output_cols,\n",
    "                np.mean(ensemble_mu, axis=0),\n",
    "                np.sqrt(aleatoric),\n",
    "                np.sqrt(epistemic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Use Monte Carlo dropout with the Gaussian model to compute aleatoric and epistemic uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte_carlo_steps = 10\n",
    "\n",
    "ensemble_mu, ensemble_var = model.predict_monte_carlo(x_test,\n",
    "                                                      monte_carlo_steps,\n",
    "                                                      scaler=y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_epistemic = np.var(ensemble_mu, axis=0)\n",
    "ensemble_aleatoric = np.mean(ensemble_var, axis=0)\n",
    "ensemble_mean = np.mean(ensemble_mu, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:guess]",
   "language": "python",
   "name": "conda-env-guess-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
