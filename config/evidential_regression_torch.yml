save_loc: "/glade/work/$USER/repos/miles-guess/miles-guess/results/"
seed: 1000

data:
  dataset_path: /glade/work/schreck/repos/miles-guess/miles-guess/applications/torch_dataset/dataset.py
  dataset_name: "CustomDataset"
  input_cols: &input_cols
   - 'wind_speed:10_m:m_s-1'
   - 'potential_temperature_skin_change:10_m:K_m-1'
   - 'bulk_richardson:10_m:None'
   - 'mixing_ratio_skin_change:2_m:g_kg-1_m-1'
  output_cols: &output_cols
    - 'friction_velocity:surface:m_s-1'
  split_params:
    flat_seed: 42  # Example seed
    data_seed: 0   # Example data split seed
  split_ratios:
    train_size: 0.9  # Train/Test split
    valid_size: 0.885  # Train/Valid split within the training set
  scaler_x:
    params:
      copy: true
      with_mean: true
      with_std: true
    type: quantile
  scaler_y:
    params:
      copy: true
      with_mean: true
      with_std: true
    type: normalize
  batch_size: &batch_size 9163 # Example batch size
  data_path: '/glade/p/cisl/aiml/ai2es/surfacelayer/cabauw_derived_data_20210720.csv'

trainer:
    mode: fsdp # none, ddp, fsdp
    training_metric: "valid_loss"
    train_batch_size: *batch_size
    valid_batch_size: *batch_size
    batches_per_epoch: 500 # Set to 0 to use len(dataloader)
    valid_batches_per_epoch: 0
    learning_rate: 0.0015285262808755972
    weight_decay: 0.0009378550509012784
    start_epoch: 0
    epochs: 100
    amp: False
    grad_accum_every: 1
    grad_max_norm: 1.0
    thread_workers: 4
    valid_thread_workers: 4
    stopping_patience: 5
    skip_validation: False
    load_weights: False
    load_optimizer: False
    use_scheduler: True
    # scheduler: {'scheduler_type': 'cosine-annealing', first_cycle_steps: 500, cycle_mult: 6.0, max_lr: 5.0e-04, min_lr: 5.0e-07, warmup_steps: 499, gamma: 0.7}
    scheduler: {scheduler_type: plateau, mode: min, factor: 0.1, patience: 2, cooldown: 2, min_lr: 1.0e-07, verbose: true, threshold: 1.0e-04}
    # scheduler: {'scheduler_type': 'lambda'}
  
model:
  input_size: *input_cols  # Reference to data:input_cols
  output_size: *output_cols  # Reference to data:output_cols
  layer_size: [1057, 1057, 1057, 1057, 1057]  # Example block sizes
  dr: [0.263, 0.263, 0.263, 0.263, 0.263]  # Dropout rates
  batch_norm: False  # Whether to use batch normalization
  lng: True  # Use the evidential layer (True) or not (False)

train_loss:
  tol: 1e-8  # Tolerance parameter for loss calculations
  coef: 0.1  # Factor for the evidence regularizer
  reduction: "mean"  # Reduction method for the loss calculation

valid_loss:
  tol: 1e-8  # Tolerance parameter for loss calculations
  coef: 0.1  # Factor for the evidence regularizer
  reduction: "mean"  # Reduction method for the loss calculation

pbs: #derecho
    conda: "holodec"
    project: "NAML0001"
    job_name: "xformer"
    walltime: "24:00:00"
    nodes: 8
    ncpus: 64
    ngpus: 4
    mem: '480GB'
    queue: 'preempt'
    
# pbs: # casper
#     conda: "/glade/work/schreck/miniconda3/envs/evidential"
#     job_name: 'latlon'
#     nodes: 1
#     ncpus: 8
#     ngpus: 1
#     mem: '128GB'
#     walltime: '12:00:00'
#     gpu_type: 'a100'
#     cpu_type: 'milan'
#     project: 'NAML0001'
#     queue: 'casper'
